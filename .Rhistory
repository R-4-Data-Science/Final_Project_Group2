# Assuming 'am' (transmission: 0 = automatic, 1 = manual) as the binary response variable
X <- mtcars$mpg
# Call the cutoff_value_function2 to evaluate 'prevalence' across different cutoffs
cutoff_value_function2(X, y, "accuracy")
# Call the cutoff_value_function2 to evaluate 'prevalence' across different cutoffs
cutoff_value_function2(X, y, "sensitivity")
# Call the cutoff_value_function2 to evaluate 'prevalence' across different cutoffs
cutoff_value_function2(X, y, "false discovery rate")
plot_logistic_curve(X,y)
logistic_regression(X, y)
# Call the cutoff_value_function2 to evaluate 'prevalence' across different cutoffs
cutoff_value_function2(X, y, "diagnostic odds ratio")
# Prepare the data
# Assuming 'am' (transmission: 0 = automatic, 1 = manual) as the binary response variable
X <- mtcars$wt
# Call the cutoff_value_function2 to evaluate 'prevalence' across different cutoffs
cutoff_value_function2(X, y, "diagnostic odds ratio")
# Prepare the data
X <- mtcars$wt
y <- mtcars$am
Bootstrap_function(alpha = 0.5, B = 20, X = X, y = y)
Bootstrap_function <- function(alpha, B = 20, X, y) {
# n is the length of the vector beta_estimates that we computed earlier, but I repeated the calculation here.
X_matrix <- cbind(1, X)
logistic_regression_result <- logistic_regression(X_matrix, y)
beta_estimates <- logistic_regression_result$OptimizedCoefficients
n <- length(beta_estimates)
boot_mean <- rep(NA, B)
for (i in 1:B) {
beta_estimates_star <- beta_estimates[sample(1:n, replace = TRUE)]
boot_mean[i] <- mean(beta_estimates_star)
}
confidence_int <- quantile(boot_mean, c((alpha/2), 1-(alpha/2)))
return(confidence_int)
}
Bootstrap_function(alpha = 0.5, B = 20, X = X, y = y)
Bootstrap_function(alpha = 0.2, B = 20, X = X, y = y)
# Prepare the data
X <- mtcars$gears
y <- mtcars$am
Bootstrap_function(alpha = 0.2, B = 20, X = X, y = y)
Bootstrap_function(alpha = 0.05, B = 20, X = X, y = y)
Bootstrap_function <- function(alpha, B = 20, X, y) {
# Perform logistic regression
X_matrix <- cbind(1, X)  # Add intercept
logistic_result <- logistic_regression(X_matrix, y)
beta_estimates <- logistic_result$OptimizedCoefficients
# Number of predictors including intercept
num_predictors <- ncol(X_matrix)
# Initialize matrix to store bootstrap means for each predictor
boot_means <- matrix(NA, nrow = B, ncol = num_predictors)
for (i in 1:B) {
# Sample indices with replacement
sample_indices <- sample(1:nrow(X_matrix), replace = TRUE)
# Perform logistic regression on the bootstrap sample
bootstrap_result <- logistic_regression(X_matrix[sample_indices, ], y[sample_indices])
boot_means[i, ] <- bootstrap_result$OptimizedCoefficients
}
# Compute confidence intervals for each predictor
confidence_intervals <- apply(boot_means, 2, function(column) quantile(column, c(alpha/2, 1 - alpha/2)))
return(confidence_intervals)
}
Bootstrap_function(alpha = 0.05, B = 20, X = X, y = y)
logistic_regression(X = cbind(mtcars$mpg, mtcars$wt), y = mtcars$am)
# Call the cutoff_value_function2 to evaluate 'prevalence' across different cutoffs
cutoff_value_function2(X = mtcars$wt, y = mtcars$am, "diagnostic odds ratio")
cutoff_value_function2(X = mtcars$wt, y = mtcars$am, "diagnostic odds ratio")
cutoff_value_function <- function(X, y, metric) {
# Extracting the coefficients as a vector
beta_vector <- as.numeric(beta_estimates$OptimizedCoefficients)
# Now use beta_vector in your logistic function
predicted_probs <- logistic_function(beta_vector, cbind(1, X))
# Initialize vectors for metrics
metric_values <- rep(NA, 9)
cutoffs <- seq(0.1, 0.9, by = 0.1)
for (i in 1:length(cutoffs)) {
cutoff <- cutoffs[i]
predictions <- ifelse(predicted_probs > cutoff, 1, 0)
# Calculate metrics based on predictions
TP <- sum(predictions == 1 & y == 1)
TN <- sum(predictions == 0 & y == 0)
FP <- sum(predictions == 1 & y == 0)
FN <- sum(predictions == 0 & y == 1)
if (metric == "accuracy") {
metric_values[i] <- (TP + TN) / (TP + TN + FP + FN)
} else if (metric == "sensitivity") {
metric_values[i] <- TP / (TP + FN)
} else if (metric == "specificity") {
metric_values[i] <- TN / (TN + FP)
} else if (metric == "false discovery rate") {
metric_values[i] <- FP / (TP + FP)
} else if (metric == "diagnostic odds ratio") {
metric_values[i] <- (TP / FP) / (FN / TN)
} else if (metric == "prevalence") {
metric_values[i] <- sum(y) / length(y)
}
}
# Plot the specified metric
plot(cutoffs, metric_values, type = "b", xlab = "Cut-off Values", ylab = metric,
main = paste(metric, "plot"), pch = 16, col = "blue")
}
# Call the cutoff_value_function2 to evaluate 'prevalence' across different cutoffs
cutoff_value_function(X = mtcars$wt, y = mtcars$am, "diagnostic odds ratio")
# Call the cutoff_value_function2 to evaluate 'prevalence' across different cutoffs
cutoff_value_function(X = mtcars$wt, y = mtcars$am, "diagnostic odds ratio")
# Call the cutoff_value_function2 to evaluate 'prevalence' across different cutoffs
cutoff_value_function(X = mtcars$wt, y = mtcars$am, "accuracy")
# Call the cutoff_value_function2 to evaluate 'prevalence' across different cutoffs
cutoff_value_function(X = mtcars$wt, y = mtcars$am, "sensitivity")
# Logistic Function
# Purpose: Computes logistic function for given predictors X and coefficients beta.
# How it Works: Calculates p_i using formula p_i = 1 / (1 + exp(-X^T * beta)), returning predicted probabilities for each observation.
logistic_function <- function(beta, X) {
# pi = 1 / (1 + exp(-xi^T * beta))
return(1 / (1 + exp(-X %*% beta)))
}
# Cost Function (Calculating the sum in the argmin equation)
# Purpose: Calculates the cost for the current set of coefficients beta.
# How it Works: Implements logisticfunction as the negative sum of log-likelihood across all observations.
cost_function <- function(beta, X, y) {
p <- logistic_function(beta, X)
# -sum(yi * log(pi) + (1 - yi) * log(1 - pi))
cost <- -sum(y * log(p + 1e-9) + (1 - y) * log(1 - p + 1e-9))  # Adding a small constant to avoid log(0)
return(cost)
}
# Gradient of the Cost Function
# Purpose: Computes the gradient of the cost function with respect to beta.
# How it Works: Used in gradient descent to determine direction to adjust coefficients beta. Gradient calculated as X^T * (p - y).
gradient_function <- function(beta, X, y) {
p <- 1 / (1 + exp(-X %*% beta))
return(t(X) %*% (p - y))
}
# Logistic Regression
# This is a function presented to the user, it takes a matrix of X as predictors and y values of 0 or 1 as output variables.
# The learning rate, max iterations, and tolerance can be adjusted by the user.
# Purpose: Optimizes coefficients beta to minimize the cost function.
# How it Works: Initializes beta using least-squares formula (X^T * X)^-1 * X^T * y.
# Iteratively updates beta in the direction of negative gradient of the cost function,
# scaled by learning rate. Checks for convergence in each iteration. Returns optimized coefficients beta.
logistic_regression <- function(X, y, learning_rate = 0.01, max_iterations = 10000, tolerance = 1e-6) {
X <- as.matrix(X)
X <- cbind(1, X)
# Initial coefficient estimation using least squares
beta <- solve(t(X) %*% X) %*% t(X) %*% y
previous_cost <- Inf
# Iterative optimization process
for (iteration in 1:max_iterations) {
current_cost <- cost_function(beta, X, y)
if (!is.na(current_cost) && !is.infinite(current_cost) && abs(previous_cost - current_cost) < tolerance) {
break
}
previous_cost <- current_cost
grad <- gradient_function(beta, X, y)
beta <- beta - learning_rate * grad
}
# Predict and binarize predictions
predicted_probs <- logistic_function(beta, X)
predictions <- ifelse(predicted_probs > 0.5, 1, 0)
# Confusion matrix components
TP <- sum(predictions == 1 & y == 1)
TN <- sum(predictions == 0 & y == 0)
FP <- sum(predictions == 1 & y == 0)
FN <- sum(predictions == 0 & y == 1)
# Calculate metrics
prevalence <- sum(y) / length(y)
accuracy <- (TP + TN) / (TP + TN + FP + FN)
sensitivity <- TP / (TP + FN)
specificity <- TN / (TN + FP)
false_discovery_rate <- FP / (TP + FP)
diagnostic_odds_ratio <- (TP / FP) / (FN / TN)
# Return a list containing optimized coefficients, confusion matrix, and metrics
return(list(
OptimizedCoefficients = beta,
ConfusionMatrix = matrix(c(TP, FP, FN, TN), nrow = 2, byrow = TRUE),
Prevalence = prevalence,
Accuracy = accuracy,
Sensitivity = sensitivity,
Specificity = specificity,
FalseDiscoveryRate = false_discovery_rate,
DiagnosticOddsRatio = diagnostic_odds_ratio
))
}
plot_logistic_curve <- function(X, y) {
# Add an intercept to X
X_matrix <- cbind(1, X)
# Compute beta estimates using your logistic regression function
logistic_regression_result <- logistic_regression(X_matrix, y)
beta_estimates <- logistic_regression_result$OptimizedCoefficients
# Calculate predicted probabilities using the logistic function and beta estimates
predicted_probabilities <- logistic_function(beta_estimates, X_matrix)
# Create a data frame for plotting
plot_data <- data.frame(X = X, y = y, predicted = predicted_probabilities)
# Sorting the data for a smooth curve in plot
plot_data <- plot_data[order(plot_data$X), ]
# Plot the original data points
plot(y ~ X, data = plot_data, col = "red", xlab = "Predictor", ylab = "Binary Response",
main = "Fitted Logistic Curve and Original Data Points",
pch = 19)  # Points for actual responses
# Add the logistic regression line
lines(plot_data$X, plot_data$predicted, type = "l", col = "blue", lwd = 2)
}
plot_logistic_curve(X, mtcars$am)
# Logistic Function
# Purpose: Computes logistic function for given predictors X and coefficients beta.
# How it Works: Calculates p_i using formula p_i = 1 / (1 + exp(-X^T * beta)), returning predicted probabilities for each observation.
logistic_function <- function(beta, X) {
# pi = 1 / (1 + exp(-xi^T * beta))
return(1 / (1 + exp(-X %*% beta)))
}
# Cost Function (Calculating the sum in the argmin equation)
# Purpose: Calculates the cost for the current set of coefficients beta.
# How it Works: Implements logisticfunction as the negative sum of log-likelihood across all observations.
cost_function <- function(beta, X, y) {
p <- logistic_function(beta, X)
# -sum(yi * log(pi) + (1 - yi) * log(1 - pi))
cost <- -sum(y * log(p + 1e-9) + (1 - y) * log(1 - p + 1e-9))  # Adding a small constant to avoid log(0)
return(cost)
}
# Gradient of the Cost Function
# Purpose: Computes the gradient of the cost function with respect to beta.
# How it Works: Used in gradient descent to determine direction to adjust coefficients beta. Gradient calculated as X^T * (p - y).
gradient_function <- function(beta, X, y) {
p <- 1 / (1 + exp(-X %*% beta))
return(t(X) %*% (p - y))
}
# Logistic Regression
# This is a function presented to the user, it takes a matrix of X as predictors and y values of 0 or 1 as output variables.
# The learning rate, max iterations, and tolerance can be adjusted by the user.
# Purpose: Optimizes coefficients beta to minimize the cost function.
# How it Works: Initializes beta using least-squares formula (X^T * X)^-1 * X^T * y.
# Iteratively updates beta in the direction of negative gradient of the cost function,
# scaled by learning rate. Checks for convergence in each iteration. Returns optimized coefficients beta.
logistic_regression <- function(X, y, learning_rate = 0.01, max_iterations = 10000, tolerance = 1e-6) {
X <- as.matrix(X)
X <- cbind(1, X)
# Initial coefficient estimation using least squares
beta <- solve(t(X) %*% X) %*% t(X) %*% y
previous_cost <- Inf
# Iterative optimization process
for (iteration in 1:max_iterations) {
current_cost <- cost_function(beta, X, y)
if (!is.na(current_cost) && !is.infinite(current_cost) && abs(previous_cost - current_cost) < tolerance) {
break
}
previous_cost <- current_cost
grad <- gradient_function(beta, X, y)
beta <- beta - learning_rate * grad
}
# Predict and binarize predictions
predicted_probs <- logistic_function(beta, X)
predictions <- ifelse(predicted_probs > 0.5, 1, 0)
# Confusion matrix components
TP <- sum(predictions == 1 & y == 1)
TN <- sum(predictions == 0 & y == 0)
FP <- sum(predictions == 1 & y == 0)
FN <- sum(predictions == 0 & y == 1)
# Calculate metrics
prevalence <- sum(y) / length(y)
accuracy <- (TP + TN) / (TP + TN + FP + FN)
sensitivity <- TP / (TP + FN)
specificity <- TN / (TN + FP)
false_discovery_rate <- FP / (TP + FP)
diagnostic_odds_ratio <- (TP / FP) / (FN / TN)
# Return a list containing optimized coefficients, confusion matrix, and metrics
return(list(
OptimizedCoefficients = beta,
ConfusionMatrix = matrix(c(TP, FP, FN, TN), nrow = 2, byrow = TRUE),
Prevalence = prevalence,
Accuracy = accuracy,
Sensitivity = sensitivity,
Specificity = specificity,
FalseDiscoveryRate = false_discovery_rate,
DiagnosticOddsRatio = diagnostic_odds_ratio
))
}
plot_logistic_curve <- function(X, y) {
# Add an intercept to X
X_matrix <- cbind(1, X)
# Compute beta estimates using your logistic regression function
logistic_regression_result <- logistic_regression(X_matrix, y)
beta_estimates <- logistic_regression_result$OptimizedCoefficients
# Calculate predicted probabilities using the logistic function and beta estimates
predicted_probabilities <- logistic_function(beta_estimates, X_matrix)
# Create a data frame for plotting
plot_data <- data.frame(X = X, y = y, predicted = predicted_probabilities)
# Sorting the data for a smooth curve in plot
plot_data <- plot_data[order(plot_data$X), ]
# Plot the original data points
plot(y ~ X, data = plot_data, col = "red", xlab = "Predictor", ylab = "Binary Response",
main = "Fitted Logistic Curve and Original Data Points",
pch = 19)  # Points for actual responses
# Add the logistic regression line
lines(plot_data$X, plot_data$predicted, type = "l", col = "blue", lwd = 2)
}
#gpt link for creating cutsom logistic regression and plotting functions: https://chat.openai.com/share/2827b3e8-a2c6-4c1d-ab39-a42235cb1deb
#gpt link for additional editing, and the addition of the matrix and other metrics to the logistic regression function
# https://chat.openai.com/share/39e19490-5083-4928-a5ef-46666b1759e4
# EXAMPLE TO BE USED
data(mtcars)
# Prepare the data
X <- cbind(mtcars$mpg, mtcars$wt)
y <- mtcars$am
#Call the function
logistic_regression(X = cbind(mtcars$mpg, mtcars$wt), y = mtcars$am)
beta_estimates <- logistic_regression(X_matrix, y)
# EXAMPLE TO BE USED
data(mtcars)
plot_logistic_curve(mtcars$wt, mtcars$am)
# Logistic Function
# Purpose: Computes logistic function for given predictors X and coefficients beta.
# How it Works: Calculates p_i using formula p_i = 1 / (1 + exp(-X^T * beta)), returning predicted probabilities for each observation.
logistic_function <- function(beta, X) {
# pi = 1 / (1 + exp(-xi^T * beta))
return(1 / (1 + exp(-X %*% beta)))
}
# Cost Function (Calculating the sum in the argmin equation)
# Purpose: Calculates the cost for the current set of coefficients beta.
# How it Works: Implements logisticfunction as the negative sum of log-likelihood across all observations.
cost_function <- function(beta, X, y) {
p <- logistic_function(beta, X)
# -sum(yi * log(pi) + (1 - yi) * log(1 - pi))
cost <- -sum(y * log(p + 1e-9) + (1 - y) * log(1 - p + 1e-9))  # Adding a small constant to avoid log(0)
return(cost)
}
# Gradient of the Cost Function
# Purpose: Computes the gradient of the cost function with respect to beta.
# How it Works: Used in gradient descent to determine direction to adjust coefficients beta. Gradient calculated as X^T * (p - y).
gradient_function <- function(beta, X, y) {
p <- 1 / (1 + exp(-X %*% beta))
return(t(X) %*% (p - y))
}
# Logistic Regression
# This is a function presented to the user, it takes a matrix of X as predictors and y values of 0 or 1 as output variables.
# The learning rate, max iterations, and tolerance can be adjusted by the user.
# Purpose: Optimizes coefficients beta to minimize the cost function.
# How it Works: Initializes beta using least-squares formula (X^T * X)^-1 * X^T * y.
# Iteratively updates beta in the direction of negative gradient of the cost function,
# scaled by learning rate. Checks for convergence in each iteration. Returns optimized coefficients beta.
logistic_regression <- function(X, y, learning_rate = 0.01, max_iterations = 10000, tolerance = 1e-6) {
X <- as.matrix(X)
X <- cbind(1, X)
# Initial coefficient estimation using least squares
beta <- solve(t(X) %*% X) %*% t(X) %*% y
previous_cost <- Inf
# Iterative optimization process
for (iteration in 1:max_iterations) {
current_cost <- cost_function(beta, X, y)
if (!is.na(current_cost) && !is.infinite(current_cost) && abs(previous_cost - current_cost) < tolerance) {
break
}
previous_cost <- current_cost
grad <- gradient_function(beta, X, y)
beta <- beta - learning_rate * grad
}
# Predict and binarize predictions
predicted_probs <- logistic_function(beta, X)
predictions <- ifelse(predicted_probs > 0.5, 1, 0)
# Confusion matrix components
TP <- sum(predictions == 1 & y == 1)
TN <- sum(predictions == 0 & y == 0)
FP <- sum(predictions == 1 & y == 0)
FN <- sum(predictions == 0 & y == 1)
# Calculate metrics
prevalence <- sum(y) / length(y)
accuracy <- (TP + TN) / (TP + TN + FP + FN)
sensitivity <- TP / (TP + FN)
specificity <- TN / (TN + FP)
false_discovery_rate <- FP / (TP + FP)
diagnostic_odds_ratio <- (TP / FP) / (FN / TN)
# Return a list containing optimized coefficients, confusion matrix, and metrics
return(list(
OptimizedCoefficients = beta,
ConfusionMatrix = matrix(c(TP, FP, FN, TN), nrow = 2, byrow = TRUE),
Prevalence = prevalence,
Accuracy = accuracy,
Sensitivity = sensitivity,
Specificity = specificity,
FalseDiscoveryRate = false_discovery_rate,
DiagnosticOddsRatio = diagnostic_odds_ratio
))
}
# EXAMPLE TO BE USED
data(mtcars)
logistic_regression(X = cbind(mtcars$mpg, mtcars$wt), y = mtcars$am)
plot_logistic_curve <- function(X, y) {
# Add an intercept to X
X_matrix <- cbind(1, X)
# Compute beta estimates using your logistic regression function
logistic_regression_result <- logistic_regression(X_matrix, y)
beta_estimates <- logistic_regression_result$OptimizedCoefficients
# Calculate predicted probabilities using the logistic function and beta estimates
predicted_probabilities <- logistic_function(beta_estimates, X_matrix)
# Create a data frame for plotting
plot_data <- data.frame(X = X, y = y, predicted = predicted_probabilities)
# Sorting the data for a smooth curve in plot
plot_data <- plot_data[order(plot_data$X), ]
# Plot the original data points
plot(y ~ X, data = plot_data, col = "red", xlab = "Predictor", ylab = "Binary Response",
main = "Fitted Logistic Curve and Original Data Points",
pch = 19)  # Points for actual responses
# Add the logistic regression line
lines(plot_data$X, plot_data$predicted, type = "l", col = "blue", lwd = 2)
}
# EXAMPLE TO BE USED
data(mtcars)
plot_logistic_curve(mtcars$wt, mtcars$am)
plot_logistic_curve(mtcars$gears, mtcars$am)
plot_logistic_curve(mtcars$gears, mtcars$wt)
plot_logistic_curve(mtcars$gears, mtcars$am)
plot_logistic_curve(mtcars$wt, mtcars$am)
plot_logistic_curve <- function(X, y) {
# Add an intercept to X
X_matrix <- cbind(1, X)
# Compute beta estimates using your logistic regression function
logistic_regression_result <- logistic_regression(X_matrix, y)
beta_estimates <- logistic_regression_result$OptimizedCoefficients
# Calculate predicted probabilities using the logistic function and beta estimates
predicted_probabilities <- logistic_function(beta_estimates, X_matrix)
# Create a data frame for plotting
plot_data <- data.frame(X = X, y = y, predicted = predicted_probabilities)
# Sorting the data for a smooth curve in plot
plot_data <- plot_data[order(plot_data$X), ]
# Plot the original data points
plot(y ~ X, data = plot_data, col = "red", xlab = "Predictor", ylab = "Binary Response",
main = "Fitted Logistic Curve and Original Data Points",
pch = 19)  # Points for actual responses
# Add the logistic regression line
lines(plot_data$X, plot_data$predicted, type = "l", col = "blue", lwd = 2)
}
plot_logistic_curve <- function(X, y) {
# Add an intercept to X
X_matrix <- cbind(1, X)
# Compute beta estimates using your logistic regression function
logistic_regression_result <- logistic_regression(X_matrix, y)
beta_estimates <- logistic_regression_result$OptimizedCoefficients
# Calculate predicted probabilities using the logistic function and beta estimates
predicted_probabilities <- logistic_function(beta_estimates, X_matrix)
# Create a data frame for plotting
plot_data <- data.frame(X = X, y = y, predicted = predicted_probabilities)
# Sorting the data for a smooth curve in plot
plot_data <- plot_data[order(plot_data$X), ]
# Plot the original data points
plot(y ~ X, data = plot_data, col = "red", xlab = "Predictor", ylab = "Binary Response",
main = "Fitted Logistic Curve and Original Data Points",
pch = 19)  # Points for actual responses
# Add the logistic regression line
lines(plot_data$X, plot_data$predicted, type = "l", col = "blue", lwd = 2)
}
plot_logistic_curve(mtcars$wt, mtcars$am)
plot_logistic_curve(mtcars$wt, mtcars$am)
Bootstrap_function <- function(alpha, B = 20, X, y) {
# n is the length of the vector beta_estimates that we computed earlier, but I repeated the calculation here.
X_matrix <- cbind(1, X)
logistic_regression_result <- logistic_regression(X_matrix, y)
beta_estimates <- logistic_regression_result$OptimizedCoefficients
n <- length(beta_estimates)
boot_mean <- rep(NA, B)
for (i in 1:B) {
beta_estimates_star <- beta_estimates[sample(1:n, replace = TRUE)]
boot_mean[i] <- mean(beta_estimates_star)
}
confidence_int <- quantile(boot_mean, c((alpha/2), 1-(alpha/2)))
return(confidence_int)
}
# Example Bootstrap Function
data(mtcars)
# Prepare the data
X <- mtcars$gears
y <- mtcars$am
Bootstrap_function(alpha = 0.05, B = 20, X = X, y = y)
cutoff_value_function <- function(X, y, metric) {
# Extracting the coefficients as a vector
beta_vector <- as.numeric(beta_estimates$OptimizedCoefficients)
# Now use beta_vector in your logistic function
predicted_probs <- logistic_function(beta_vector, cbind(1, X))
# Initialize vectors for metrics
metric_values <- rep(NA, 9)
cutoffs <- seq(0.1, 0.9, by = 0.1)
for (i in 1:length(cutoffs)) {
cutoff <- cutoffs[i]
predictions <- ifelse(predicted_probs > cutoff, 1, 0)
# Calculate metrics based on predictions
TP <- sum(predictions == 1 & y == 1)
TN <- sum(predictions == 0 & y == 0)
FP <- sum(predictions == 1 & y == 0)
FN <- sum(predictions == 0 & y == 1)
if (metric == "accuracy") {
metric_values[i] <- (TP + TN) / (TP + TN + FP + FN)
} else if (metric == "sensitivity") {
metric_values[i] <- TP / (TP + FN)
} else if (metric == "specificity") {
metric_values[i] <- TN / (TN + FP)
} else if (metric == "false discovery rate") {
metric_values[i] <- FP / (TP + FP)
} else if (metric == "diagnostic odds ratio") {
metric_values[i] <- (TP / FP) / (FN / TN)
} else if (metric == "prevalence") {
metric_values[i] <- sum(y) / length(y)
}
}
# Plot the specified metric
plot(cutoffs, metric_values, type = "b", xlab = "Cut-off Values", ylab = metric,
main = paste(metric, "plot"), pch = 16, col = "blue")
}
# Example for cutoff_value_function
data(mtcars)
# Prepare the data
# Assuming 'am' (transmission: 0 = automatic, 1 = manual) as the binary response variable
X <- mtcars$wt
y <- mtcars$am
# Call the cutoff_value_function2 to evaluate 'prevalence' across different cutoffs
cutoff_value_function(X = mtcars$wt, y = mtcars$am, "sensitivity")
