if (abs(previous_cost - current_cost) < tolerance) {
break
}
previous_cost <- current_cost
# Update beta based on the gradient
grad <- gradient_function(beta, X, y)
beta <- beta - learning_rate * grad
}
return(beta)
}
# Example using our created functions
data(mtcars)
# Prepare the data
X <- mtcars$mpg
y <- mtcars$am
# Convert X to a matrix and add an intercept
X_matrix <- cbind(1, X)
beta_estimates <- logistic_regression(X_matrix, y)
# Print the estimated coefficients
beta_estimates
#Actual Logistic Regression in R for comparison
data_glm <- data.frame(y = y, X = X)
# Fit Logistic Regression model using glm()
model_glm <- glm(y ~ X, data = data_glm, family = binomial(link = "logit"))
# Display the summary of the model
summary(model_glm)
# To get the coefficients
coefficients(model_glm)
rep(0, ncol(X))
beta <- solve(t(X) %*% X) %*% t(X) %*% y
#Actual Logistic Regression in R for comparison
data_glm <- data.frame(y = y, X = X)
# Logistic Function
logistic_function <- function(beta, X) {
# pi = 1 / (1 + exp(-xi^T * beta))
return(1 / (1 + exp(-X %*% beta)))
}
# Cost Function (Calculating the sum in the argmin equation)
cost_function <- function(beta, X, y) {
p <- logistic_function(beta, X)
# -sum(yi * log(pi) + (1 - yi) * log(1 - pi))
return(-sum(y * log(p) + (1 - y) * log(1 - p)))
}
# Gradient of the Cost Function
gradient_function <- function(beta, X, y) {
p <- 1 / (1 + exp(-X %*% beta))
return(t(X) %*% (p - y))
}
# Gradient Descent for Logistic Regression
gradient_descent <- function(X, y, learning_rate = 0.01, max_iterations = 1000, tolerance = 1e-6) {
beta <- solve(t(X) %*% X) %*% t(X) %*% y
previous_cost <- Inf
for (iteration in 1:max_iterations) {t
current_cost <- cost_function(beta, X, y)
if (abs(previous_cost - current_cost) < tolerance) {
break
}
previous_cost <- current_cost
# Update beta based on the gradient
grad <- gradient_function(beta, X, y)
beta <- beta - learning_rate * grad
}
return(beta)
}
# Example using our created functions
data(mtcars)
# Prepare the data
X <- mtcars$mpg
y <- mtcars$am
# Convert X to a matrix and add an intercept
X_matrix <- cbind(1, X)
beta_estimates <- logistic_regression(X_matrix, y)
# Print the estimated coefficients
beta_estimates
#Actual Logistic Regression in R for comparison
data_glm <- data.frame(y = y, X = X)
# Fit Logistic Regression model using glm()
model_glm <- glm(y ~ X, data = data_glm, family = binomial(link = "logit"))
# Display the summary of the model
summary(model_glm)
# To get the coefficients
coefficients(model_glm)
# Logistic Function
# Purpose: Computes logistic function for given predictors X and coefficients beta.
# How it Works: Calculates p_i using formula p_i = 1 / (1 + exp(-X^T * beta)), returning predicted probabilities for each observation.
logistic_function <- function(beta, X) {
# pi = 1 / (1 + exp(-xi^T * beta))
return(1 / (1 + exp(-X %*% beta)))
}
# Cost Function (Calculating the sum in the argmin equation)
# Purpose: Calculates the cost for the current set of coefficients beta.
# How it Works: Implements logistic regression cost function as the negative sum of log-likelihood across all observations.
cost_function <- function(beta, X, y) {
p <- logistic_function(beta, X)
# -sum(yi * log(pi) + (1 - yi) * log(1 - pi))
return(-sum(y * log(p) + (1 - y) * log(1 - p)))
}
# Gradient of the Cost Function
# Purpose: Computes the gradient of the cost function with respect to beta.
# How it Works: Used in gradient descent to determine direction to adjust coefficients beta. Gradient calculated as X^T * (p - y).
gradient_function <- function(beta, X, y) {
p <- 1 / (1 + exp(-X %*% beta))
return(t(X) %*% (p - y))
}
# Gradient Descent for Logistic Regression
# Purpose: Optimizes coefficients beta to minimize the cost function.
# How it Works: Initializes beta using least-squares formula (X^T * X)^-1 * X^T * y.
# Iteratively updates beta in the direction of negative gradient of the cost function,
# scaled by learning rate. Checks for convergence in each iteration. Returns optimized coefficients beta.
gradient_descent <- function(X, y, learning_rate = 0.01, max_iterations = 1000, tolerance = 1e-6) {
beta <- solve(t(X) %*% X) %*% t(X) %*% y
previous_cost <- Inf
for (iteration in 1:max_iterations) {t
current_cost <- cost_function(beta, X, y)
if (abs(previous_cost - current_cost) < tolerance) {
break
}
previous_cost <- current_cost
# Update beta based on the gradient
grad <- gradient_function(beta, X, y)
beta <- beta - learning_rate * grad
}
return(beta)
}
data(mtcars)
# Prepare the data
X <- mtcars$mpg
y <- mtcars$am
# Convert X to a matrix and add an intercept
X_matrix <- cbind(1, X)
beta_estimates <- logistic_regression(X_matrix, y)
# Print the estimated coefficients
beta_estimates
#Actual Logistic Regression in R for comparison
data_glm <- data.frame(y = y, X = X)
# Fit Logistic Regression model using glm()
model_glm <- glm(y ~ X, data = data_glm, family = binomial(link = "logit"))
# Display the summary of the model
summary(model_glm)
# To get the coefficients
coefficients(model_glm)
# Calculate Predicted Probabilities
predicted_probabilities <- logistic_function(beta_hat, X)
# Example using our created functions
data(mtcars)
# Prepare the data
X <- mtcars$mpg
y <- mtcars$am
# Convert X to a matrix and add an intercept
X_matrix <- cbind(1, X)
beta_estimates <- logistic_regression(X_matrix, y)
# Print the estimated coefficients
beta_estimates
predicted_probabilities <- logistic_function(beta_hat, X)
# Assuming X is your matrix of predictors (with an intercept column if needed) and beta_hat is your estimated coefficients
# Calculate Predicted Probabilities
predicted_probabilities <- logistic_function(beta_hat, X)
predicted_probabilities <- logistic_function(beta_hat, X)
lenght(beta_hat)
length(beta_hat)
length(X)
beta_hat <- gradient_descent(X, y)
X
y
# Example using our created functions
data(mtcars)
# Prepare the data
X <- mtcars$mpg
y <- mtcars$am
beta_estimates <- logistic_regression(X, y)
beta_estimates <- logistic_regression(X_matrix, y)
# Print the estimated coefficients
beta_estimates
# Create a data frame with var2 in ascending order for predictions
Predicted_data <- data.frame(var2 = seq(min(df$var2), max(df$var2), length.out = 500))
# Create a data frame with var2 in ascending order for predictions
Predicted_data <- data.frame(var2 = seq(min(df$var2), max(df$var2), length.out = 500))
?mtcars
X_matrix <- as.matrix(cbind(1, X))
# Calculate predicted probabilities using the logistic_function and beta_estimates
predicted_probabilities <- logistic_function(beta_estimates, X_matrix)
# Create a data frame for plotting
plot_data <- data.frame(mpg = X, am = y, predicted = predicted_probabilities)
# Sorting the data for a smooth curve in plot
plot_data <- plot_data[order(plot_data$mpg), ]
plot(am ~ mpg, data = mtcars, col = "red", xlab = "Miles Per Gallon (mpg)", ylab = "Automatic Transmission (am)",
main = "Fitted Logistic Curve and Original Data Points",
pch = 19)  # Points for actual responses
# Add the logistic regression line
lines(plot_data$mpg, plot_data$predicted, type = "l", col = "blue", lwd = 2)
plot_logistic_curve <- function(X, y) {
# Add an intercept to X
X_matrix <- cbind(1, X)
# Compute beta estimates using your logistic regression function
beta_estimates <- logistic_regression(X_matrix, y)
# Calculate predicted probabilities using the logistic function and beta estimates
predicted_probabilities <- logistic_function(beta_estimates, X_matrix)
# Create a data frame for plotting
plot_data <- data.frame(X = X, y = y, predicted = predicted_probabilities)
# Sorting the data for a smooth curve in plot
plot_data <- plot_data[order(plot_data$X), ]
# Plot the original data points
plot(y ~ X, data = plot_data, col = "red", xlab = "Predictor", ylab = "Binary Response",
main = "Fitted Logistic Curve and Original Data Points",
pch = 19)  # Points for actual responses
# Add the logistic regression line
lines(plot_data$X, plot_data$predicted, type = "l", col = "blue", lwd = 2)
}
plot_logistic_curve(mtcars$mpg, mtcars$am)
plot_logistic_curve(mtcars$mpg, mtcars$am)
?mtcars
plot_logistic_curve(mtcars$wt, mtcars$am)
?logistic_regression
logistic_regression <- function(X, y, learning_rate = 0.01, max_iterations = 1000, tolerance = 1e-6) {
beta <- solve(t(X) %*% X) %*% t(X) %*% y
previous_cost <- Inf
for (iteration in 1:max_iterations) {t
current_cost <- cost_function(beta, X, y)
if (abs(previous_cost - current_cost) < tolerance) {
break
}
previous_cost <- current_cost
# Update beta based on the gradient
grad <- gradient_function(beta, X, y)
beta <- beta - learning_rate * grad
}
return(beta)
}
plot_logistic_curve <- function(X, y) {
# Add an intercept to X
X_matrix <- cbind(1, X)
# Compute beta estimates using your logistic regression function
beta_estimates <- logistic_regression(X_matrix, y)
# Calculate predicted probabilities using the logistic function and beta estimates
predicted_probabilities <- logistic_function(beta_estimates, X_matrix)
# Create a data frame for plotting
plot_data <- data.frame(X = X, y = y, predicted = predicted_probabilities)
# Sorting the data for a smooth curve in plot
plot_data <- plot_data[order(plot_data$X), ]
# Plot the original data points
plot(y ~ X, data = plot_data, col = "red", xlab = "Predictor", ylab = "Binary Response",
main = "Fitted Logistic Curve and Original Data Points",
pch = 19)  # Points for actual responses
# Add the logistic regression line
lines(plot_data$X, plot_data$predicted, type = "l", col = "blue", lwd = 2)
}
# Example using our created functions
data(mtcars)
# Prepare the data
X <- mtcars$mpg
y <- mtcars$am
# Convert X to a matrix and add an intercept
X_matrix <- cbind(1, X)
beta_estimates <- logistic_regression(X_matrix, y)
gradient_descent <- function(X, y, learning_rate = 0.01, max_iterations = 1000, tolerance = 1e-6) {
beta <- solve(t(X) %*% X) %*% t(X) %*% y
previous_cost <- Inf
for (iteration in 1:max_iterations) {t
current_cost <- cost_function(beta, X, y)
if (abs(previous_cost - current_cost) < tolerance) {
break
}
previous_cost <- current_cost
# Update beta based on the gradient
grad <- gradient_function(beta, X, y)
beta <- beta - learning_rate * grad
}
return(beta)
}
# Logistic Function
# Purpose: Computes logistic function for given predictors X and coefficients beta.
# How it Works: Calculates p_i using formula p_i = 1 / (1 + exp(-X^T * beta)), returning predicted probabilities for each observation.
logistic_function <- function(beta, X) {
# pi = 1 / (1 + exp(-xi^T * beta))
return(1 / (1 + exp(-X %*% beta)))
}
# Cost Function (Calculating the sum in the argmin equation)
# Purpose: Calculates the cost for the current set of coefficients beta.
# How it Works: Implements logistic regression cost function as the negative sum of log-likelihood across all observations.
cost_function <- function(beta, X, y) {
p <- logistic_function(beta, X)
# -sum(yi * log(pi) + (1 - yi) * log(1 - pi))
return(-sum(y * log(p) + (1 - y) * log(1 - p)))
}
# Gradient of the Cost Function
# Purpose: Computes the gradient of the cost function with respect to beta.
# How it Works: Used in gradient descent to determine direction to adjust coefficients beta. Gradient calculated as X^T * (p - y).
gradient_function <- function(beta, X, y) {
p <- 1 / (1 + exp(-X %*% beta))
return(t(X) %*% (p - y))
}
# Gradient Descent for Logistic Regression
# Purpose: Optimizes coefficients beta to minimize the cost function.
# How it Works: Initializes beta using least-squares formula (X^T * X)^-1 * X^T * y.
# Iteratively updates beta in the direction of negative gradient of the cost function,
# scaled by learning rate. Checks for convergence in each iteration. Returns optimized coefficients beta.
gradient_descent <- function(X, y, learning_rate = 0.01, max_iterations = 1000, tolerance = 1e-6) {
beta <- solve(t(X) %*% X) %*% t(X) %*% y
previous_cost <- Inf
for (iteration in 1:max_iterations) {t
current_cost <- cost_function(beta, X, y)
if (abs(previous_cost - current_cost) < tolerance) {
break
}
previous_cost <- current_cost
# Update beta based on the gradient
grad <- gradient_function(beta, X, y)
beta <- beta - learning_rate * grad
}
return(beta)
}
# Example using our created functions
data(mtcars)
# Prepare the data
X <- mtcars$mpg
y <- mtcars$am
# Convert X to a matrix and add an intercept
X_matrix <- cbind(1, X)
beta_estimates <- logistic_regression(X_matrix, y)
logistic_regression <- function(X, y, learning_rate = 0.01, max_iterations = 1000, tolerance = 1e-6) {
beta <- solve(t(X) %*% X) %*% t(X) %*% y
previous_cost <- Inf
for (iteration in 1:max_iterations) {t
current_cost <- cost_function(beta, X, y)
if (abs(previous_cost - current_cost) < tolerance) {
break
}
previous_cost <- current_cost
# Update beta based on the gradient
grad <- gradient_function(beta, X, y)
beta <- beta - learning_rate * grad
}
logistic_regression <- function(X, y, learning_rate = 0.01, max_iterations = 1000, tolerance = 1e-6) {
beta <- solve(t(X) %*% X) %*% t(X) %*% y
previous_cost <- Inf
for (iteration in 1:max_iterations) {t
current_cost <- cost_function(beta, X, y)
if (abs(previous_cost - current_cost) < tolerance) {
break
}
previous_cost <- current_cost
# Update beta based on the gradient
grad <- gradient_function(beta, X, y)
beta <- beta - learning_rate * grad
}
return(beta)
}
beta_estimates <- logistic_regression(X_matrix, y)
# Logistic Function
# Purpose: Computes logistic function for given predictors X and coefficients beta.
# How it Works: Calculates p_i using formula p_i = 1 / (1 + exp(-X^T * beta)), returning predicted probabilities for each observation.
logistic_function <- function(beta, X) {
# pi = 1 / (1 + exp(-xi^T * beta))
return(1 / (1 + exp(-X %*% beta)))
}
# Cost Function (Calculating the sum in the argmin equation)
# Purpose: Calculates the cost for the current set of coefficients beta.
# How it Works: Implements logistic regression cost function as the negative sum of log-likelihood across all observations.
cost_function <- function(beta, X, y) {
p <- logistic_function(beta, X)
# -sum(yi * log(pi) + (1 - yi) * log(1 - pi))
return(-sum(y * log(p) + (1 - y) * log(1 - p)))
}
# Gradient of the Cost Function
# Purpose: Computes the gradient of the cost function with respect to beta.
# How it Works: Used in gradient descent to determine direction to adjust coefficients beta. Gradient calculated as X^T * (p - y).
gradient_function <- function(beta, X, y) {
p <- 1 / (1 + exp(-X %*% beta))
return(t(X) %*% (p - y))
}
# Logistic Regression
# Purpose: Optimizes coefficients beta to minimize the cost function.
# How it Works: Initializes beta using least-squares formula (X^T * X)^-1 * X^T * y.
# Iteratively updates beta in the direction of negative gradient of the cost function,
# scaled by learning rate. Checks for convergence in each iteration. Returns optimized coefficients beta.
logistic_regression <- function(X, y, learning_rate = 0.01, max_iterations = 1000, tolerance = 1e-6) {
beta <- solve(t(X) %*% X) %*% t(X) %*% y
previous_cost <- Inf
for (iteration in 1:max_iterations) {t
current_cost <- cost_function(beta, X, y)
if (abs(previous_cost - current_cost) < tolerance) {
break
}
previous_cost <- current_cost
# Update beta based on the gradient
grad <- gradient_function(beta, X, y)
beta <- beta - learning_rate * grad
}
return(beta)
}
# Example using our created functions
data(mtcars)
# Prepare the data
X <- mtcars$mpg
y <- mtcars$am
# Convert X to a matrix and add an intercept
X_matrix <- cbind(1, X)
beta_estimates <- logistic_regression(X_matrix, y)
# Print the estimated coefficients
beta_estimates
# X_matrix <- as.matrix(cbind(1, X))
#
# # Calculate predicted probabilities using the logistic_function and beta_estimates
# predicted_probabilities <- logistic_function(beta_estimates, X_matrix)
plot_logistic_curve <- function(X, y) {
# Add an intercept to X
X_matrix <- cbind(1, X)
# Compute beta estimates using your logistic regression function
beta_estimates <- logistic_regression(X_matrix, y)
# Calculate predicted probabilities using the logistic function and beta estimates
predicted_probabilities <- logistic_function(beta_estimates, X_matrix)
# Create a data frame for plotting
plot_data <- data.frame(X = X, y = y, predicted = predicted_probabilities)
# Sorting the data for a smooth curve in plot
plot_data <- plot_data[order(plot_data$X), ]
# Plot the original data points
plot(y ~ X, data = plot_data, col = "red", xlab = "Predictor", ylab = "Binary Response",
main = "Fitted Logistic Curve and Original Data Points",
pch = 19)  # Points for actual responses
# Add the logistic regression line
lines(plot_data$X, plot_data$predicted, type = "l", col = "blue", lwd = 2)
}
#Actual Logistic Regression in R for comparison
data_glm <- data.frame(y = y, X = X)
# Fit Logistic Regression model using glm()
model_glm <- glm(y ~ X, data = data_glm, family = binomial(link = "logit"))
# Display the summary of the model
summary(model_glm)
# To get the coefficients
coefficients(model_glm)
# Logistic Function
# Purpose: Computes logistic function for given predictors X and coefficients beta.
# How it Works: Calculates p_i using formula p_i = 1 / (1 + exp(-X^T * beta)), returning predicted probabilities for each observation.
logistic_function <- function(beta, X) {
# pi = 1 / (1 + exp(-xi^T * beta))
return(1 / (1 + exp(-X %*% beta)))
}
# Cost Function (Calculating the sum in the argmin equation)
# Purpose: Calculates the cost for the current set of coefficients beta.
# How it Works: Implements logistic regression cost function as the negative sum of log-likelihood across all observations.
cost_function <- function(beta, X, y) {
p <- logistic_function(beta, X)
# -sum(yi * log(pi) + (1 - yi) * log(1 - pi))
return(-sum(y * log(p) + (1 - y) * log(1 - p)))
}
# Gradient of the Cost Function
# Purpose: Computes the gradient of the cost function with respect to beta.
# How it Works: Used in gradient descent to determine direction to adjust coefficients beta. Gradient calculated as X^T * (p - y).
gradient_function <- function(beta, X, y) {
p <- 1 / (1 + exp(-X %*% beta))
return(t(X) %*% (p - y))
}
# Logistic Regression
# Purpose: Optimizes coefficients beta to minimize the cost function.
# How it Works: Initializes beta using least-squares formula (X^T * X)^-1 * X^T * y.
# Iteratively updates beta in the direction of negative gradient of the cost function,
# scaled by learning rate. Checks for convergence in each iteration. Returns optimized coefficients beta.
logistic_regression <- function(X, y, learning_rate = 0.01, max_iterations = 1000, tolerance = 1e-6) {
beta <- solve(t(X) %*% X) %*% t(X) %*% y
previous_cost <- Inf
for (iteration in 1:max_iterations) {t
current_cost <- cost_function(beta, X, y)
if (abs(previous_cost - current_cost) < tolerance) {
break
}
previous_cost <- current_cost
# Update beta based on the gradient
grad <- gradient_function(beta, X, y)
beta <- beta - learning_rate * grad
}
return(beta)
}
# Example using our created functions
data(mtcars)
# Prepare the data
X <- mtcars$mpg
y <- mtcars$am
# Convert X to a matrix and add an intercept
X_matrix <- cbind(1, X)
beta_estimates <- logistic_regression(X_matrix, y)
# Print the estimated coefficients
beta_estimates
# X_matrix <- as.matrix(cbind(1, X))
#
# # Calculate predicted probabilities using the logistic_function and beta_estimates
# predicted_probabilities <- logistic_function(beta_estimates, X_matrix)
plot_logistic_curve <- function(X, y) {
# Add an intercept to X
X_matrix <- cbind(1, X)
# Compute beta estimates using your logistic regression function
beta_estimates <- logistic_regression(X_matrix, y)
# Calculate predicted probabilities using the logistic function and beta estimates
predicted_probabilities <- logistic_function(beta_estimates, X_matrix)
# Create a data frame for plotting
plot_data <- data.frame(X = X, y = y, predicted = predicted_probabilities)
# Sorting the data for a smooth curve in plot
plot_data <- plot_data[order(plot_data$X), ]
# Plot the original data points
plot(y ~ X, data = plot_data, col = "red", xlab = "Predictor", ylab = "Binary Response",
main = "Fitted Logistic Curve and Original Data Points",
pch = 19)  # Points for actual responses
# Add the logistic regression line
lines(plot_data$X, plot_data$predicted, type = "l", col = "blue", lwd = 2)
}
#Actual Logistic Regression in R for comparison
data_glm <- data.frame(y = y, X = X)
# Fit Logistic Regression model using glm()
model_glm <- glm(y ~ X, data = data_glm, family = binomial(link = "logit"))
# Display the summary of the model
summary(model_glm)
# To get the coefficients
coefficients(model_glm)
