

# Logistic Function
# Purpose: Computes logistic function for given predictors X and coefficients beta.
# How it Works: Calculates p_i using formula p_i = 1 / (1 + exp(-X^T * beta)), returning predicted probabilities for each observation.
logistic_function <- function(beta, X) {
  # pi = 1 / (1 + exp(-xi^T * beta))
  return(1 / (1 + exp(-X %*% beta)))
}

# Cost Function (Calculating the sum in the argmin equation)
# Purpose: Calculates the cost for the current set of coefficients beta.
# How it Works: Implements logisticfunction as the negative sum of log-likelihood across all observations.
cost_function <- function(beta, X, y) {
  p <- logistic_function(beta, X)
  # -sum(yi * log(pi) + (1 - yi) * log(1 - pi))
  cost <- -sum(y * log(p + 1e-9) + (1 - y) * log(1 - p + 1e-9))  # Adding a small constant to avoid log(0)
  return(cost)
}


# Gradient of the Cost Function
# Purpose: Computes the gradient of the cost function with respect to beta.
# How it Works: Used in gradient descent to determine direction to adjust coefficients beta. Gradient calculated as X^T * (p - y).
gradient_function <- function(beta, X, y) {
  p <- 1 / (1 + exp(-X %*% beta))
  return(t(X) %*% (p - y))
}

# Logistic Regression
# This is a function presented to the user, it takes an X Vector as predictor and y values of 0 or 1 as output variables.
# The learning rate, max iterations, and tolerance can be adjusted by the user.
# Purpose: Optimizes coefficients beta to minimize the cost function.
# How it Works: Initializes beta using least-squares formula (X^T * X)^-1 * X^T * y. 
# Iteratively updates beta in the direction of negative gradient of the cost function,
# scaled by learning rate. Checks for convergence in each iteration. Returns optimized coefficients beta.
logistic_regression <- function(X, y, learning_rate = 0.01, max_iterations = 10000, tolerance = 1e-6) {
  beta <- solve(t(X) %*% X) %*% t(X) %*% y
  previous_cost <- Inf
  
  for (iteration in 1:max_iterations) {t
    current_cost <- cost_function(beta, X, y)

    # if (abs(previous_cost - current_cost) < tolerance) {
    #   break
    # }
    if (!is.na(current_cost) && !is.infinite(current_cost) && abs(previous_cost - current_cost) < tolerance) {
      break
    }
    previous_cost <- current_cost
    
    # Update beta based on the gradient
    grad <- gradient_function(beta, X, y)
    beta <- beta - learning_rate * grad
  }
  
  return(beta)
}


# Example using our created functions
  data(mtcars)

  # Prepare the data
  X <- mtcars$wt
  y <- mtcars$am

  # Convert X to a matrix and add an intercept
  X_matrix <- cbind(1, X)

  beta_estimates <- logistic_regression(X_matrix, y)

  # Print the estimated coefficients
  beta_estimates
  
  
  
  
  plot_logistic_curve <- function(X, y) {
    # Add an intercept to X
    X_matrix <- cbind(1, X)
    
    # Compute beta estimates using your logistic regression function
    beta_estimates <- logistic_regression(X_matrix, y)
    
    # Calculate predicted probabilities using the logistic function and beta estimates
    predicted_probabilities <- logistic_function(beta_estimates, X_matrix)
    
    # Create a data frame for plotting
    plot_data <- data.frame(X = X, y = y, predicted = predicted_probabilities)
    
    # Sorting the data for a smooth curve in plot
    plot_data <- plot_data[order(plot_data$X), ]
    
    # Plot the original data points
    plot(y ~ X, data = plot_data, col = "red", xlab = "Predictor", ylab = "Binary Response", 
         main = "Fitted Logistic Curve and Original Data Points",
         pch = 19)  # Points for actual responses
    
    # Add the logistic regression line
    lines(plot_data$X, plot_data$predicted, type = "l", col = "blue", lwd = 2)
  }
  
  
  plot_logistic_curve(mtcars$wt, mtcars$am)
  
  
  
#Actual Logistic Regression in R for comparison
  data_glm <- data.frame(y = y, X = X)

  # Fit Logistic Regression model using glm()
  model_glm <- glm(y ~ X, data = data_glm, family = binomial(link = "logit"))

  # Display the summary of the model
  summary(model_glm)

  # To get the coefficients
  coefficients(model_glm)

